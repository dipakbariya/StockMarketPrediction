{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "web_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvwUUlkq83kG"
      },
      "source": [
        "**Web Scraping With snscrape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lha8YpVAEAFZ",
        "outputId": "2e983c7e-2bf3-4e12-af53-1c152a01fce9"
      },
      "source": [
        "# pip install snscrape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.3.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.2.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UGRgH-8FPW5"
      },
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "tweets_list2 = []\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#TSLA since:2020-01-01 until:2021-09-22 lang:en').get_items()):\n",
        "    # if i>5000:\n",
        "    #     break\n",
        "    tweets_list2.append([tweet.date, tweet.content])\n",
        "    \n",
        "# Creating a dataframe from the tweets list above\n",
        "dlf = pd.DataFrame(tweets_list2, columns=['Datetime', 'Text'])\n",
        "dlf.to_csv('/content/TSLA.csv', sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD6i7TguHde4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGvwsIcPxk1-",
        "outputId": "9bd13a47-134c-42c1-cc02-0f12e809ff5c"
      },
      "source": [
        "print(df[0:15])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    Datetime                                               Text\n",
            "0  2021-09-20 16:17:31+00:00  The market is selling off as we expected. Look...\n",
            "1  2021-09-20 15:30:22+00:00  Check out my #GOOGL analysis on @TradingView: ...\n",
            "2  2021-09-20 15:14:51+00:00  Taking profits on my #GOOG / #GOOGL short now ...\n",
            "3  2021-09-20 13:18:40+00:00  $SNOW #SNOW\\nI’d consider Calls over 324\\nI’d ...\n",
            "4  2021-09-20 07:32:10+00:00  OVHcloud has been pushing itself as a safe ste...\n",
            "5  2021-09-20 00:54:29+00:00  Ad prices have surged on Amazon as competition...\n",
            "6  2021-09-19 17:00:00+00:00  #GOOGL - GOOGL - TradingView - https://t.co/l0...\n",
            "7  2021-09-18 17:02:19+00:00  Market traders use the weekend and dive into s...\n",
            "8  2021-09-18 16:27:55+00:00  Covered Calls Cash Flow Strategy: \\n Ticker: $...\n",
            "9  2021-09-18 16:27:55+00:00  Covered Calls Cash Flow Strategy: \\n Ticker: $...\n",
            "10 2021-09-18 05:23:47+00:00  Google is the internet !! #google #googl #goog...\n",
            "11 2021-09-18 04:53:47+00:00  Google...What Can I Say !! #google #googl #sto...\n",
            "12 2021-09-17 21:23:47+00:00  Google is the internet !! #google #googl #goog...\n",
            "13 2021-09-17 20:53:47+00:00  Google...What Can I Say !! #google #googl #sto...\n",
            "14 2021-09-17 15:34:14+00:00  You missed #AAPL (Apple)\\nYou missed #GOOGL (G...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnKHaFt6JtDQ",
        "outputId": "dd7bf746-622c-4557-eec3-60987a243c5b"
      },
      "source": [
        "sep.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(323, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bp7G93DKHsp"
      },
      "source": [
        "month = [\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "since_date = [\"2020-01-01\",\"2020-02-01\",\"2020-03-01\",\"2020-04-01\",\"2020-05-01\",\"2020-06-01\",\"2020-07-01\",\"2020-08-01\",\"2020-09-01\",\"2020-10-01\",\"2020-11-01\",\"2020-12-01\",\"2021-01-01\",\"2021-02-01\",\"2021-03-01\",\"2021-04-01\",\"2021-05-01\",\"2021-06-01\",\"2021-07-01\",\"2021-08-01\",\"2021-09-01\"]\n",
        "until_date = [\"2020-02-01\",\"2020-03-01\",\"2020-04-01\",\"2020-05-01\",\"2020-06-01\",\"2020-07-01\",\"2020-08-01\",\"2020-09-01\",\"2020-10-01\",\"2020-11-01\",\"2020-12-01\",\"2021-01-01\",\"2021-02-01\",\"2021-03-01\",\"2021-04-01\",\"2021-05-01\",\"2021-06-01\",\"2021-07-01\",\"2021-08-01\",\"2021-09-01\",\"2021-09-21\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q0FDupPZI8p"
      },
      "source": [
        "month = [\"jan_GOO\",\"feb_GOO\"]\n",
        "since_date = [\"2020-01-01\",\"2020-02-01\"]\n",
        "until_date = [\"2020-02-01\",\"2020-03-01\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2MmVym0XvaT"
      },
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "\n",
        "for j in range(len(month)):\n",
        "  since=since_date[j]\n",
        "  until=until_date[j]\n",
        "\n",
        "  tweets_list2 = []\n",
        "  data=pd.DataFrame()\n",
        "  # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "  for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#GOOGL since:'+str(since)+'until:'+str(until)+'lang:en').get_items()):        # if i>5000:\n",
        "        #     break\n",
        "    tweets_list2.append([tweet.date, tweet.content])\n",
        "        \n",
        "    # Creating a dataframe from the tweets list above\n",
        "  data = pd.DataFrame(tweets_list2, columns=['Datetime', 'Text'])\n",
        "  data.to_csv('/content/'+str(month[j])+'.csv', sep=',', index=False)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R17PGicJZ3_f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSWrCNEd6z-k"
      },
      "source": [
        "Define Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "455uaqoI62Mk"
      },
      "source": [
        "def scraper():\n",
        "  # !pip install snscrape\n",
        "  import snscrape.modules.twitter as sntwitter\n",
        "  import pandas as pd\n",
        "\n",
        "  # Creating list to append tweet data to\n",
        "\n",
        "  since=input(\"Enter staring date in YYYY-MM-DD format.\")\n",
        "  until=input(\"Enter end date in YYYY-MM-DD fromat.\")\n",
        "  hashtag=input(\"Enter hashtag.\")\n",
        "\n",
        "  tweets_list2 = []\n",
        "  data=pd.DataFrame()\n",
        "  # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "  for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#'+str(hashtag)+' since:'+str(since)+'until:'+str(until)+'lang:en').get_items()):        # if i>5000:\n",
        "        #     break\n",
        "    tweets_list2.append([tweet.date, tweet.content])\n",
        "        \n",
        "    # Creating a dataframe from the tweets list above\n",
        "  data = pd.DataFrame(tweets_list2, columns=['Datetime', 'Text'])\n",
        "  return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaF5Y2xU7_PS",
        "outputId": "1b538121-7722-4dbe-aec6-3aaba7ec805c"
      },
      "source": [
        "data = scraper()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: snscrape in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.2.6)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Enter staring date in YYYY-MM-DD format.2021-09-21\n",
            "Enter end date in YYYY-MM-DD fromat.2021-09-22\n",
            "Enter hashtag.MSFT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA6oZph98dKR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}